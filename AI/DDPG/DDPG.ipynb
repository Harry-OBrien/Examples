{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "608b9b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7094c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  8\n",
      "Size of Action Space ->  2\n",
      "Max Value of Action ->  1.0\n",
      "Min Value of Action ->  -1.0\n"
     ]
    }
   ],
   "source": [
    "# problem = \"Pendulum-v0\"\n",
    "problem = \"LunarLanderContinuous-v2\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24597792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82ea5a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "    @tf.function\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + gamma * target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch, training=True)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4709793",
   "metadata": {},
   "source": [
    "# Create actor and critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1784a75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-3e-3, maxval=3e-3)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(128, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(128, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(num_actions, activation=\"linear\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Our upper bound is 2.0 for Pendulum.\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(128, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(128, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07354159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return np.squeeze(legal_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0c81f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 09:22:50.457081: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "std_dev = 0.3\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.0015\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 600\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.98\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "buffer = Buffer(50000, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d348ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 09:22:52.418 python[18440:497703] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fae3edfdf00>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-29 09:22:52.419 python[18440:497703] Warning: Expected min height of view: (<NSButton: 0x7fae420471c0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-29 09:22:52.424 python[18440:497703] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fae42047c40>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-29 09:22:52.426 python[18440:497703] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fae4204a450>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> -143.57117872719678\n",
      "Episode * 1 * Avg Reward is ==> -143.62239395344278\n",
      "Episode * 2 * Avg Reward is ==> -324.2397237582259\n",
      "Episode * 3 * Avg Reward is ==> -368.34861438862845\n",
      "Episode * 4 * Avg Reward is ==> -427.79785894830763\n",
      "Episode * 5 * Avg Reward is ==> -452.1324575365574\n",
      "Episode * 6 * Avg Reward is ==> -449.2811621768324\n",
      "Episode * 7 * Avg Reward is ==> -536.7568141628435\n",
      "Episode * 8 * Avg Reward is ==> -490.87714576432677\n",
      "Episode * 9 * Avg Reward is ==> -468.5832149955074\n",
      "Episode * 10 * Avg Reward is ==> -471.4442268830551\n",
      "Episode * 11 * Avg Reward is ==> -470.4613022063022\n",
      "Episode * 12 * Avg Reward is ==> -451.1878132869792\n",
      "Episode * 13 * Avg Reward is ==> -429.01388484489144\n",
      "Episode * 14 * Avg Reward is ==> -421.2657765233734\n",
      "Episode * 15 * Avg Reward is ==> -416.39409448748586\n",
      "Episode * 16 * Avg Reward is ==> -412.17929920494726\n",
      "Episode * 17 * Avg Reward is ==> -405.37880069826133\n",
      "Episode * 18 * Avg Reward is ==> -404.12445819545536\n",
      "Episode * 19 * Avg Reward is ==> -393.11550866706114\n",
      "Episode * 20 * Avg Reward is ==> -382.92947097220525\n",
      "Episode * 21 * Avg Reward is ==> -373.3434343843312\n",
      "Episode * 22 * Avg Reward is ==> -367.5523318375054\n",
      "Episode * 23 * Avg Reward is ==> -390.36383104342104\n",
      "Episode * 24 * Avg Reward is ==> -389.40191563210885\n",
      "Episode * 25 * Avg Reward is ==> -392.9961090786197\n",
      "Episode * 26 * Avg Reward is ==> -406.3836042813565\n",
      "Episode * 27 * Avg Reward is ==> -406.545580383041\n",
      "Episode * 28 * Avg Reward is ==> -419.9980351730993\n",
      "Episode * 29 * Avg Reward is ==> -430.65537578248683\n",
      "Episode * 30 * Avg Reward is ==> -430.1746976370118\n",
      "Episode * 31 * Avg Reward is ==> -430.67984251176733\n",
      "Episode * 32 * Avg Reward is ==> -429.79962254244253\n",
      "Episode * 33 * Avg Reward is ==> -433.99588642172415\n",
      "Episode * 34 * Avg Reward is ==> -437.777194912574\n",
      "Episode * 35 * Avg Reward is ==> -428.8168314072359\n",
      "Episode * 36 * Avg Reward is ==> -421.831118151434\n",
      "Episode * 37 * Avg Reward is ==> -414.3608486442405\n",
      "Episode * 38 * Avg Reward is ==> -407.5695224914622\n",
      "Episode * 39 * Avg Reward is ==> -401.63191421892736\n",
      "Episode * 40 * Avg Reward is ==> -401.0776051028478\n",
      "Episode * 41 * Avg Reward is ==> -397.21854952536404\n",
      "Episode * 42 * Avg Reward is ==> -382.8550981279417\n",
      "Episode * 43 * Avg Reward is ==> -374.1417947979279\n",
      "Episode * 44 * Avg Reward is ==> -361.22272499999474\n",
      "Episode * 45 * Avg Reward is ==> -346.9604901679607\n",
      "Episode * 46 * Avg Reward is ==> -339.1179795123865\n",
      "Episode * 47 * Avg Reward is ==> -314.76376229229754\n",
      "Episode * 48 * Avg Reward is ==> -314.4773855243384\n",
      "Episode * 49 * Avg Reward is ==> -307.53558376677967\n",
      "Episode * 50 * Avg Reward is ==> -298.7927907020091\n",
      "Episode * 51 * Avg Reward is ==> -290.64771321942754\n",
      "Episode * 52 * Avg Reward is ==> -287.93005136345164\n",
      "Episode * 53 * Avg Reward is ==> -288.0949953542776\n",
      "Episode * 54 * Avg Reward is ==> -283.216858494333\n",
      "Episode * 55 * Avg Reward is ==> -277.254512985394\n",
      "Episode * 56 * Avg Reward is ==> -272.81376556357674\n",
      "Episode * 57 * Avg Reward is ==> -268.47547208135717\n",
      "Episode * 58 * Avg Reward is ==> -263.836546746199\n",
      "Episode * 59 * Avg Reward is ==> -262.20578725819394\n",
      "Episode * 60 * Avg Reward is ==> -260.99827742919376\n",
      "Episode * 61 * Avg Reward is ==> -259.8732134160765\n",
      "Episode * 62 * Avg Reward is ==> -256.8752311731513\n",
      "Episode * 63 * Avg Reward is ==> -237.5357698564093\n",
      "Episode * 64 * Avg Reward is ==> -231.26922335988647\n",
      "Episode * 65 * Avg Reward is ==> -223.54833302816488\n",
      "Episode * 66 * Avg Reward is ==> -206.87065994405165\n",
      "Episode * 67 * Avg Reward is ==> -200.4559555742711\n",
      "Episode * 68 * Avg Reward is ==> -183.7436642419064\n",
      "Episode * 69 * Avg Reward is ==> -168.53836738564164\n",
      "Episode * 70 * Avg Reward is ==> -161.8882605561755\n",
      "Episode * 71 * Avg Reward is ==> -151.261662861706\n",
      "Episode * 72 * Avg Reward is ==> -144.74489564414372\n",
      "Episode * 73 * Avg Reward is ==> -134.78492080948456\n",
      "Episode * 74 * Avg Reward is ==> -123.53110882329804\n",
      "Episode * 75 * Avg Reward is ==> -125.35666413032747\n",
      "Episode * 76 * Avg Reward is ==> -124.99893777165751\n",
      "Episode * 77 * Avg Reward is ==> -125.19586075370293\n",
      "Episode * 78 * Avg Reward is ==> -123.54518509945197\n",
      "Episode * 79 * Avg Reward is ==> -123.31293731442001\n",
      "Episode * 80 * Avg Reward is ==> -123.22496720642395\n",
      "Episode * 81 * Avg Reward is ==> -125.98004526281257\n",
      "Episode * 82 * Avg Reward is ==> -125.63230699413532\n",
      "Episode * 83 * Avg Reward is ==> -125.00673099969838\n",
      "Episode * 84 * Avg Reward is ==> -124.83479310359931\n",
      "Episode * 85 * Avg Reward is ==> -128.53020091253546\n",
      "Episode * 86 * Avg Reward is ==> -129.24767492413787\n",
      "Episode * 87 * Avg Reward is ==> -127.79007473179149\n",
      "Episode * 88 * Avg Reward is ==> -129.0412524589552\n",
      "Episode * 89 * Avg Reward is ==> -132.77715180826686\n",
      "Episode * 90 * Avg Reward is ==> -132.00758246665578\n",
      "Episode * 91 * Avg Reward is ==> -131.70220009815887\n",
      "Episode * 92 * Avg Reward is ==> -132.52241432899467\n",
      "Episode * 93 * Avg Reward is ==> -132.1850803295527\n",
      "Episode * 94 * Avg Reward is ==> -132.42834263189965\n",
      "Episode * 95 * Avg Reward is ==> -133.9149563167153\n",
      "Episode * 96 * Avg Reward is ==> -132.79416091060006\n",
      "Episode * 97 * Avg Reward is ==> -134.45916651495227\n",
      "Episode * 98 * Avg Reward is ==> -132.69577155656248\n",
      "Episode * 99 * Avg Reward is ==> -133.2073619046424\n",
      "Episode * 100 * Avg Reward is ==> -133.68426001761242\n",
      "Episode * 101 * Avg Reward is ==> -133.80973838640475\n",
      "Episode * 102 * Avg Reward is ==> -134.62961259050195\n",
      "Episode * 103 * Avg Reward is ==> -133.29275678251298\n",
      "Episode * 104 * Avg Reward is ==> -133.39987830264\n",
      "Episode * 105 * Avg Reward is ==> -131.78230103155693\n",
      "Episode * 106 * Avg Reward is ==> -138.4220701646894\n",
      "Episode * 107 * Avg Reward is ==> -137.90256195215625\n",
      "Episode * 108 * Avg Reward is ==> -137.78827785627723\n",
      "Episode * 109 * Avg Reward is ==> -133.9765871959816\n",
      "Episode * 110 * Avg Reward is ==> -134.07936175666768\n",
      "Episode * 111 * Avg Reward is ==> -136.8500656273629\n",
      "Episode * 112 * Avg Reward is ==> -136.52311815289204\n",
      "Episode * 113 * Avg Reward is ==> -134.354478441185\n",
      "Episode * 114 * Avg Reward is ==> -133.4766666908693\n",
      "Episode * 115 * Avg Reward is ==> -132.1298867510819\n",
      "Episode * 116 * Avg Reward is ==> -131.1242876761564\n",
      "Episode * 117 * Avg Reward is ==> -130.74311396193485\n",
      "Episode * 118 * Avg Reward is ==> -131.9963183824591\n",
      "Episode * 119 * Avg Reward is ==> -131.12173887413655\n",
      "Episode * 120 * Avg Reward is ==> -136.37610766479025\n",
      "Episode * 121 * Avg Reward is ==> -137.7122226512533\n",
      "Episode * 122 * Avg Reward is ==> -138.84388014212368\n",
      "Episode * 123 * Avg Reward is ==> -139.6292933312305\n",
      "Episode * 124 * Avg Reward is ==> -135.67993528917145\n",
      "Episode * 125 * Avg Reward is ==> -135.10242903827265\n",
      "Episode * 126 * Avg Reward is ==> -134.97000872160365\n",
      "Episode * 127 * Avg Reward is ==> -135.84164504049835\n",
      "Episode * 128 * Avg Reward is ==> -135.45413757113874\n",
      "Episode * 129 * Avg Reward is ==> -135.38879257610716\n",
      "Episode * 130 * Avg Reward is ==> -135.4631267391691\n",
      "Episode * 131 * Avg Reward is ==> -136.06580375515318\n",
      "Episode * 132 * Avg Reward is ==> -135.7175767478107\n",
      "Episode * 133 * Avg Reward is ==> -135.18184767414374\n",
      "Episode * 134 * Avg Reward is ==> -135.65093747478636\n",
      "Episode * 135 * Avg Reward is ==> -134.87394328348347\n",
      "Episode * 136 * Avg Reward is ==> -133.8224018223875\n",
      "Episode * 137 * Avg Reward is ==> -133.37877809616108\n",
      "Episode * 138 * Avg Reward is ==> -134.05204712313844\n",
      "Episode * 139 * Avg Reward is ==> -133.4574521961499\n",
      "Episode * 140 * Avg Reward is ==> -131.44078181955737\n",
      "Episode * 141 * Avg Reward is ==> -128.6661485283054\n",
      "Episode * 142 * Avg Reward is ==> -128.3088440529819\n",
      "Episode * 143 * Avg Reward is ==> -129.05132340483064\n",
      "Episode * 144 * Avg Reward is ==> -126.73197339767091\n",
      "Episode * 145 * Avg Reward is ==> -127.63198592791866\n",
      "Episode * 146 * Avg Reward is ==> -121.78852755159998\n",
      "Episode * 147 * Avg Reward is ==> -121.27363278243556\n",
      "Episode * 148 * Avg Reward is ==> -122.12580825601466\n",
      "Episode * 149 * Avg Reward is ==> -126.17691044504072\n",
      "Episode * 150 * Avg Reward is ==> -125.21987087946437\n",
      "Episode * 151 * Avg Reward is ==> -122.38343384219006\n",
      "Episode * 152 * Avg Reward is ==> -121.64106597808625\n",
      "Episode * 153 * Avg Reward is ==> -122.15695764307047\n",
      "Episode * 154 * Avg Reward is ==> -123.39960942670913\n",
      "Episode * 155 * Avg Reward is ==> -123.6266390123642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 156 * Avg Reward is ==> -123.40412586324378\n",
      "Episode * 157 * Avg Reward is ==> -123.59938740771676\n",
      "Episode * 158 * Avg Reward is ==> -123.05392671778364\n",
      "Episode * 159 * Avg Reward is ==> -123.34676779310253\n",
      "Episode * 160 * Avg Reward is ==> -119.84474761932411\n",
      "Episode * 161 * Avg Reward is ==> -119.3197355246991\n",
      "Episode * 162 * Avg Reward is ==> -116.45529352014846\n",
      "Episode * 163 * Avg Reward is ==> -115.55926178085531\n",
      "Episode * 164 * Avg Reward is ==> -116.8747218790912\n",
      "Episode * 165 * Avg Reward is ==> -117.5327256059949\n",
      "Episode * 166 * Avg Reward is ==> -116.1592958597087\n",
      "Episode * 167 * Avg Reward is ==> -115.97017544759107\n",
      "Episode * 168 * Avg Reward is ==> -117.46100117693418\n",
      "Episode * 169 * Avg Reward is ==> -117.72008158829567\n",
      "Episode * 170 * Avg Reward is ==> -117.89751505374446\n",
      "Episode * 171 * Avg Reward is ==> -117.70316570367686\n",
      "Episode * 172 * Avg Reward is ==> -114.74049204989247\n",
      "Episode * 173 * Avg Reward is ==> -115.28666084798223\n",
      "Episode * 174 * Avg Reward is ==> -115.34650289097581\n",
      "Episode * 175 * Avg Reward is ==> -114.12486576677588\n",
      "Episode * 176 * Avg Reward is ==> -114.67756209035115\n",
      "Episode * 177 * Avg Reward is ==> -113.84772694336853\n",
      "Episode * 178 * Avg Reward is ==> -113.14330118404291\n",
      "Episode * 179 * Avg Reward is ==> -110.93687971952582\n",
      "Episode * 180 * Avg Reward is ==> -113.1453730207317\n",
      "Episode * 181 * Avg Reward is ==> -116.25719130863149\n",
      "Episode * 182 * Avg Reward is ==> -115.87297323757794\n",
      "Episode * 183 * Avg Reward is ==> -116.44064807475903\n",
      "Episode * 184 * Avg Reward is ==> -118.03536734445552\n",
      "Episode * 185 * Avg Reward is ==> -117.09794561928871\n",
      "Episode * 186 * Avg Reward is ==> -118.22298570525038\n",
      "Episode * 187 * Avg Reward is ==> -119.31386334320544\n",
      "Episode * 188 * Avg Reward is ==> -119.01025409996555\n",
      "Episode * 189 * Avg Reward is ==> -116.9696278611596\n",
      "Episode * 190 * Avg Reward is ==> -117.21075518116982\n",
      "Episode * 191 * Avg Reward is ==> -119.68008368320986\n",
      "Episode * 192 * Avg Reward is ==> -117.68488773912088\n",
      "Episode * 193 * Avg Reward is ==> -117.8224516726013\n",
      "Episode * 194 * Avg Reward is ==> -118.14814898294631\n",
      "Episode * 195 * Avg Reward is ==> -118.25499664826971\n",
      "Episode * 196 * Avg Reward is ==> -119.18231874560902\n",
      "Episode * 197 * Avg Reward is ==> -119.68660018516888\n",
      "Episode * 198 * Avg Reward is ==> -120.09753063970916\n",
      "Episode * 199 * Avg Reward is ==> -120.13000468777632\n",
      "Episode * 200 * Avg Reward is ==> -119.56463058995905\n",
      "Episode * 201 * Avg Reward is ==> -119.50322296781812\n",
      "Episode * 202 * Avg Reward is ==> -121.5765268924877\n",
      "Episode * 203 * Avg Reward is ==> -120.74491470553293\n",
      "Episode * 204 * Avg Reward is ==> -123.32486207002157\n",
      "Episode * 205 * Avg Reward is ==> -119.54104258531856\n",
      "Episode * 206 * Avg Reward is ==> -120.61868642859534\n",
      "Episode * 207 * Avg Reward is ==> -119.68382470502843\n",
      "Episode * 208 * Avg Reward is ==> -117.68986782524507\n",
      "Episode * 209 * Avg Reward is ==> -117.45362337648253\n",
      "Episode * 210 * Avg Reward is ==> -117.38541961247033\n",
      "Episode * 211 * Avg Reward is ==> -117.71986478338158\n",
      "Episode * 212 * Avg Reward is ==> -120.60372145882857\n",
      "Episode * 213 * Avg Reward is ==> -120.74816384430733\n",
      "Episode * 214 * Avg Reward is ==> -120.2987572070136\n",
      "Episode * 215 * Avg Reward is ==> -121.24556698726761\n",
      "Episode * 216 * Avg Reward is ==> -121.82849439582296\n",
      "Episode * 217 * Avg Reward is ==> -121.87882854070904\n",
      "Episode * 218 * Avg Reward is ==> -119.80904157143716\n",
      "Episode * 219 * Avg Reward is ==> -122.70306316389379\n",
      "Episode * 220 * Avg Reward is ==> -121.95190881838502\n",
      "Episode * 221 * Avg Reward is ==> -121.23147456064387\n",
      "Episode * 222 * Avg Reward is ==> -120.48044232455997\n",
      "Episode * 223 * Avg Reward is ==> -121.52020824383199\n",
      "Episode * 224 * Avg Reward is ==> -122.77806414835955\n",
      "Episode * 225 * Avg Reward is ==> -123.46748698043902\n",
      "Episode * 226 * Avg Reward is ==> -123.42613953859681\n",
      "Episode * 227 * Avg Reward is ==> -121.43760596097385\n",
      "Episode * 228 * Avg Reward is ==> -121.03871874555698\n",
      "Episode * 229 * Avg Reward is ==> -122.32815070041188\n",
      "Episode * 230 * Avg Reward is ==> -122.38372471379849\n",
      "Episode * 231 * Avg Reward is ==> -122.40209395417483\n",
      "Episode * 232 * Avg Reward is ==> -124.90989503373518\n",
      "Episode * 233 * Avg Reward is ==> -122.93858013428556\n",
      "Episode * 234 * Avg Reward is ==> -123.37274600246792\n",
      "Episode * 235 * Avg Reward is ==> -123.21825767530959\n",
      "Episode * 236 * Avg Reward is ==> -122.25602407831983\n",
      "Episode * 237 * Avg Reward is ==> -122.58455880655583\n",
      "Episode * 238 * Avg Reward is ==> -122.91127712070275\n",
      "Episode * 239 * Avg Reward is ==> -119.66585147268697\n",
      "Episode * 240 * Avg Reward is ==> -118.39154486769617\n",
      "Episode * 241 * Avg Reward is ==> -119.69563369101142\n",
      "Episode * 242 * Avg Reward is ==> -119.00254591063758\n",
      "Episode * 243 * Avg Reward is ==> -121.09998706238574\n",
      "Episode * 244 * Avg Reward is ==> -120.43620860146174\n",
      "Episode * 245 * Avg Reward is ==> -124.79139732766251\n",
      "Episode * 246 * Avg Reward is ==> -124.37803939375165\n",
      "Episode * 247 * Avg Reward is ==> -126.06345122692214\n",
      "Episode * 248 * Avg Reward is ==> -125.4728508172324\n",
      "Episode * 249 * Avg Reward is ==> -125.66827883588948\n",
      "Episode * 250 * Avg Reward is ==> -127.52472493997007\n",
      "Episode * 251 * Avg Reward is ==> -127.25722853430268\n",
      "Episode * 252 * Avg Reward is ==> -127.4451644166962\n",
      "Episode * 253 * Avg Reward is ==> -128.33900611227259\n",
      "Episode * 254 * Avg Reward is ==> -127.79648218790808\n",
      "Episode * 255 * Avg Reward is ==> -129.0239114595388\n",
      "Episode * 256 * Avg Reward is ==> -129.29517085161757\n",
      "Episode * 257 * Avg Reward is ==> -129.40113315906387\n",
      "Episode * 258 * Avg Reward is ==> -127.7878184811389\n",
      "Episode * 259 * Avg Reward is ==> -127.45037195142099\n",
      "Episode * 260 * Avg Reward is ==> -126.60264054246275\n",
      "Episode * 261 * Avg Reward is ==> -126.14970440930679\n",
      "Episode * 262 * Avg Reward is ==> -127.09257585600972\n",
      "Episode * 263 * Avg Reward is ==> -127.60520722225624\n",
      "Episode * 264 * Avg Reward is ==> -127.21511920227204\n",
      "Episode * 265 * Avg Reward is ==> -127.63294343051675\n",
      "Episode * 266 * Avg Reward is ==> -126.63721408514296\n",
      "Episode * 267 * Avg Reward is ==> -127.75898880982731\n",
      "Episode * 268 * Avg Reward is ==> -128.64383148507378\n",
      "Episode * 269 * Avg Reward is ==> -129.67363663950346\n",
      "Episode * 270 * Avg Reward is ==> -130.22595401091786\n",
      "Episode * 271 * Avg Reward is ==> -130.41320793232632\n",
      "Episode * 272 * Avg Reward is ==> -129.66725442974035\n",
      "Episode * 273 * Avg Reward is ==> -131.91296751697678\n",
      "Episode * 274 * Avg Reward is ==> -130.97401097882016\n",
      "Episode * 275 * Avg Reward is ==> -130.45840578038778\n",
      "Episode * 276 * Avg Reward is ==> -130.7291814331051\n",
      "Episode * 277 * Avg Reward is ==> -128.8741827124893\n",
      "Episode * 278 * Avg Reward is ==> -129.11126695736294\n",
      "Episode * 279 * Avg Reward is ==> -131.79864861096442\n",
      "Episode * 280 * Avg Reward is ==> -133.6737336624968\n",
      "Episode * 281 * Avg Reward is ==> -131.19320465849347\n",
      "Episode * 282 * Avg Reward is ==> -131.62073221954347\n",
      "Episode * 283 * Avg Reward is ==> -130.48582443738903\n",
      "Episode * 284 * Avg Reward is ==> -131.00099974583134\n",
      "Episode * 285 * Avg Reward is ==> -127.21452199676476\n",
      "Episode * 286 * Avg Reward is ==> -127.19124652266653\n",
      "Episode * 287 * Avg Reward is ==> -127.17329961052255\n",
      "Episode * 288 * Avg Reward is ==> -126.63775996209775\n",
      "Episode * 289 * Avg Reward is ==> -127.27398452222835\n",
      "Episode * 290 * Avg Reward is ==> -127.20059467198939\n",
      "Episode * 291 * Avg Reward is ==> -127.38530722791822\n",
      "Episode * 292 * Avg Reward is ==> -127.97979574573542\n",
      "Episode * 293 * Avg Reward is ==> -126.72051440118693\n",
      "Episode * 294 * Avg Reward is ==> -127.59110093004597\n",
      "Episode * 295 * Avg Reward is ==> -126.14935695075314\n",
      "Episode * 296 * Avg Reward is ==> -125.48663387042006\n",
      "Episode * 297 * Avg Reward is ==> -124.5569939886611\n",
      "Episode * 298 * Avg Reward is ==> -128.84841890135374\n",
      "Episode * 299 * Avg Reward is ==> -129.34860681272934\n",
      "Episode * 300 * Avg Reward is ==> -130.00570231553584\n",
      "Episode * 301 * Avg Reward is ==> -131.14976622176613\n",
      "Episode * 302 * Avg Reward is ==> -131.8256348684648\n",
      "Episode * 303 * Avg Reward is ==> -128.90343186212462\n",
      "Episode * 304 * Avg Reward is ==> -129.30736709309036\n",
      "Episode * 305 * Avg Reward is ==> -129.40215658851605\n",
      "Episode * 306 * Avg Reward is ==> -128.77794723747175\n",
      "Episode * 307 * Avg Reward is ==> -127.86866376457706\n",
      "Episode * 308 * Avg Reward is ==> -127.30464689846602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 309 * Avg Reward is ==> -127.49886813890416\n",
      "Episode * 310 * Avg Reward is ==> -126.70614578080809\n",
      "Episode * 311 * Avg Reward is ==> -126.91531087873281\n",
      "Episode * 312 * Avg Reward is ==> -128.21266071996683\n",
      "Episode * 313 * Avg Reward is ==> -129.3598476532614\n",
      "Episode * 314 * Avg Reward is ==> -131.47176526888833\n",
      "Episode * 315 * Avg Reward is ==> -130.78213612595218\n",
      "Episode * 316 * Avg Reward is ==> -130.75660992754774\n",
      "Episode * 317 * Avg Reward is ==> -131.35829276070746\n",
      "Episode * 318 * Avg Reward is ==> -131.70633286733155\n",
      "Episode * 319 * Avg Reward is ==> -131.79721798623703\n",
      "Episode * 320 * Avg Reward is ==> -130.28806733580282\n",
      "Episode * 321 * Avg Reward is ==> -131.0136930579942\n",
      "Episode * 322 * Avg Reward is ==> -130.9890754965368\n",
      "Episode * 323 * Avg Reward is ==> -131.75227053292082\n",
      "Episode * 324 * Avg Reward is ==> -131.40589147687552\n",
      "Episode * 325 * Avg Reward is ==> -133.94625184620554\n",
      "Episode * 326 * Avg Reward is ==> -130.947706909284\n",
      "Episode * 327 * Avg Reward is ==> -130.26755129353776\n",
      "Episode * 328 * Avg Reward is ==> -131.87460233848736\n",
      "Episode * 329 * Avg Reward is ==> -129.43903148613157\n",
      "Episode * 330 * Avg Reward is ==> -128.59725584732433\n",
      "Episode * 331 * Avg Reward is ==> -128.5207607981966\n",
      "Episode * 332 * Avg Reward is ==> -127.45487513241025\n",
      "Episode * 333 * Avg Reward is ==> -127.68574950968029\n",
      "Episode * 334 * Avg Reward is ==> -127.00185489023595\n",
      "Episode * 335 * Avg Reward is ==> -127.3366543048264\n",
      "Episode * 336 * Avg Reward is ==> -129.5955969579289\n",
      "Episode * 337 * Avg Reward is ==> -130.56578535327907\n",
      "Episode * 338 * Avg Reward is ==> -130.89067597998923\n",
      "Episode * 339 * Avg Reward is ==> -130.0908225118861\n",
      "Episode * 340 * Avg Reward is ==> -134.22186058380439\n",
      "Episode * 341 * Avg Reward is ==> -134.06844190993843\n",
      "Episode * 342 * Avg Reward is ==> -133.9653127085403\n",
      "Episode * 343 * Avg Reward is ==> -136.48693101255057\n",
      "Episode * 344 * Avg Reward is ==> -136.61991216316892\n",
      "Episode * 345 * Avg Reward is ==> -135.66571764785647\n",
      "Episode * 346 * Avg Reward is ==> -137.4082227703117\n",
      "Episode * 347 * Avg Reward is ==> -138.03107534054374\n",
      "Episode * 348 * Avg Reward is ==> -138.65973955235933\n",
      "Episode * 349 * Avg Reward is ==> -138.64686283591203\n",
      "Episode * 350 * Avg Reward is ==> -139.71245514809476\n",
      "Episode * 351 * Avg Reward is ==> -139.78323928184403\n",
      "Episode * 352 * Avg Reward is ==> -140.00994656862002\n",
      "Episode * 353 * Avg Reward is ==> -138.82289765292654\n",
      "Episode * 354 * Avg Reward is ==> -143.4698728179479\n",
      "Episode * 355 * Avg Reward is ==> -143.49910356136647\n",
      "Episode * 356 * Avg Reward is ==> -143.89965120370454\n",
      "Episode * 357 * Avg Reward is ==> -143.4721533420722\n",
      "Episode * 358 * Avg Reward is ==> -142.41490725065756\n",
      "Episode * 359 * Avg Reward is ==> -143.84210521725362\n",
      "Episode * 360 * Avg Reward is ==> -144.24414022258628\n",
      "Episode * 361 * Avg Reward is ==> -144.05404486467302\n",
      "Episode * 362 * Avg Reward is ==> -144.4444962559748\n",
      "Episode * 363 * Avg Reward is ==> -143.17606191195188\n",
      "Episode * 364 * Avg Reward is ==> -143.77180024505307\n",
      "Episode * 365 * Avg Reward is ==> -143.6353915603132\n",
      "Episode * 366 * Avg Reward is ==> -146.47387473774353\n",
      "Episode * 367 * Avg Reward is ==> -147.067897008756\n",
      "Episode * 368 * Avg Reward is ==> -146.22693219422948\n",
      "Episode * 369 * Avg Reward is ==> -147.32055318515106\n",
      "Episode * 370 * Avg Reward is ==> -147.66548394716966\n",
      "Episode * 371 * Avg Reward is ==> -147.436327799568\n",
      "Episode * 372 * Avg Reward is ==> -147.89317769444935\n",
      "Episode * 373 * Avg Reward is ==> -148.23686246938098\n",
      "Episode * 374 * Avg Reward is ==> -146.44645452539905\n",
      "Episode * 375 * Avg Reward is ==> -147.8620125015562\n",
      "Episode * 376 * Avg Reward is ==> -145.9134119170534\n",
      "Episode * 377 * Avg Reward is ==> -145.5143866557172\n",
      "Episode * 378 * Avg Reward is ==> -145.16102036550643\n",
      "Episode * 379 * Avg Reward is ==> -145.99664724317904\n",
      "Episode * 380 * Avg Reward is ==> -141.94113107223902\n",
      "Episode * 381 * Avg Reward is ==> -142.7420832217751\n",
      "Episode * 382 * Avg Reward is ==> -142.33968800182592\n",
      "Episode * 383 * Avg Reward is ==> -141.57632690259106\n",
      "Episode * 384 * Avg Reward is ==> -138.05258550692062\n",
      "Episode * 385 * Avg Reward is ==> -139.49698982127825\n",
      "Episode * 386 * Avg Reward is ==> -138.05122046887732\n",
      "Episode * 387 * Avg Reward is ==> -137.95482716550106\n",
      "Episode * 388 * Avg Reward is ==> -136.55569539284815\n",
      "Episode * 389 * Avg Reward is ==> -136.36496061497786\n",
      "Episode * 390 * Avg Reward is ==> -135.23029575981565\n",
      "Episode * 391 * Avg Reward is ==> -136.633395557263\n",
      "Episode * 392 * Avg Reward is ==> -136.3703281571863\n",
      "Episode * 393 * Avg Reward is ==> -137.9657253163687\n",
      "Episode * 394 * Avg Reward is ==> -131.54345454572695\n",
      "Episode * 395 * Avg Reward is ==> -132.80517977683496\n",
      "Episode * 396 * Avg Reward is ==> -133.38498522531705\n",
      "Episode * 397 * Avg Reward is ==> -134.1245788214211\n",
      "Episode * 398 * Avg Reward is ==> -134.15245321032796\n",
      "Episode * 399 * Avg Reward is ==> -130.7878338457471\n",
      "Episode * 400 * Avg Reward is ==> -131.53694530507013\n",
      "Episode * 401 * Avg Reward is ==> -131.92532448998608\n",
      "Episode * 402 * Avg Reward is ==> -133.38661853851482\n",
      "Episode * 403 * Avg Reward is ==> -132.86497709244307\n",
      "Episode * 404 * Avg Reward is ==> -133.39029996438552\n",
      "Episode * 405 * Avg Reward is ==> -132.95405755155156\n",
      "Episode * 406 * Avg Reward is ==> -133.12922756979282\n",
      "Episode * 407 * Avg Reward is ==> -132.05304524785407\n",
      "Episode * 408 * Avg Reward is ==> -132.1058705776357\n",
      "Episode * 409 * Avg Reward is ==> -131.15003595539116\n",
      "Episode * 410 * Avg Reward is ==> -130.5539333815625\n",
      "Episode * 411 * Avg Reward is ==> -131.47942313707034\n",
      "Episode * 412 * Avg Reward is ==> -131.6482411896357\n",
      "Episode * 413 * Avg Reward is ==> -132.06516687794448\n",
      "Episode * 414 * Avg Reward is ==> -134.14847658060836\n",
      "Episode * 415 * Avg Reward is ==> -133.9848934241061\n",
      "Episode * 416 * Avg Reward is ==> -133.958447073601\n",
      "Episode * 417 * Avg Reward is ==> -135.00745382611285\n",
      "Episode * 418 * Avg Reward is ==> -134.79172171189597\n",
      "Episode * 419 * Avg Reward is ==> -134.26517676339208\n",
      "Episode * 420 * Avg Reward is ==> -134.4324710281288\n",
      "Episode * 421 * Avg Reward is ==> -134.22481953229266\n",
      "Episode * 422 * Avg Reward is ==> -133.84552804662442\n",
      "Episode * 423 * Avg Reward is ==> -134.03557742798452\n",
      "Episode * 424 * Avg Reward is ==> -137.46757077504327\n",
      "Episode * 425 * Avg Reward is ==> -136.01676427567688\n",
      "Episode * 426 * Avg Reward is ==> -136.25328358680258\n",
      "Episode * 427 * Avg Reward is ==> -136.09414975234594\n",
      "Episode * 428 * Avg Reward is ==> -136.66051843561314\n",
      "Episode * 429 * Avg Reward is ==> -134.9638712423412\n",
      "Episode * 430 * Avg Reward is ==> -135.3371546796675\n",
      "Episode * 431 * Avg Reward is ==> -133.13944668435602\n",
      "Episode * 432 * Avg Reward is ==> -133.25475297251623\n",
      "Episode * 433 * Avg Reward is ==> -132.04327292471874\n",
      "Episode * 434 * Avg Reward is ==> -132.30464848734644\n",
      "Episode * 435 * Avg Reward is ==> -131.47288116250633\n",
      "Episode * 436 * Avg Reward is ==> -130.5627475584227\n",
      "Episode * 437 * Avg Reward is ==> -130.86507513122143\n",
      "Episode * 438 * Avg Reward is ==> -130.797593104477\n",
      "Episode * 439 * Avg Reward is ==> -134.1726825446272\n",
      "Episode * 440 * Avg Reward is ==> -132.47546761743905\n",
      "Episode * 441 * Avg Reward is ==> -132.6383261700441\n",
      "Episode * 442 * Avg Reward is ==> -132.2213139919338\n",
      "Episode * 443 * Avg Reward is ==> -134.35098498217275\n",
      "Episode * 444 * Avg Reward is ==> -133.70107419951992\n",
      "Episode * 445 * Avg Reward is ==> -134.57843885576298\n",
      "Episode * 446 * Avg Reward is ==> -135.42302001990987\n",
      "Episode * 447 * Avg Reward is ==> -135.42781181615845\n",
      "Episode * 448 * Avg Reward is ==> -135.89680659241213\n",
      "Episode * 449 * Avg Reward is ==> -138.02280840526265\n",
      "Episode * 450 * Avg Reward is ==> -135.3205808584285\n",
      "Episode * 451 * Avg Reward is ==> -134.66832062908813\n",
      "Episode * 452 * Avg Reward is ==> -134.77538762828945\n",
      "Episode * 453 * Avg Reward is ==> -133.0638565474782\n",
      "Episode * 454 * Avg Reward is ==> -132.91653155877094\n",
      "Episode * 455 * Avg Reward is ==> -132.0612442457346\n",
      "Episode * 456 * Avg Reward is ==> -131.1245550159024\n",
      "Episode * 457 * Avg Reward is ==> -129.20200270158767\n",
      "Episode * 458 * Avg Reward is ==> -128.52371490695768\n",
      "Episode * 459 * Avg Reward is ==> -129.06669573952337\n",
      "Episode * 460 * Avg Reward is ==> -128.07567756216233\n",
      "Episode * 461 * Avg Reward is ==> -127.08597897912014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 462 * Avg Reward is ==> -127.60892651858413\n",
      "Episode * 463 * Avg Reward is ==> -126.91667408932904\n",
      "Episode * 464 * Avg Reward is ==> -126.82269217295614\n",
      "Episode * 465 * Avg Reward is ==> -128.05758990797523\n",
      "Episode * 466 * Avg Reward is ==> -128.03864089823537\n",
      "Episode * 467 * Avg Reward is ==> -128.02023870664496\n",
      "Episode * 468 * Avg Reward is ==> -128.33873570036502\n",
      "Episode * 469 * Avg Reward is ==> -129.02866823204295\n",
      "Episode * 470 * Avg Reward is ==> -128.26722646838422\n",
      "Episode * 471 * Avg Reward is ==> -129.69195218722479\n",
      "Episode * 472 * Avg Reward is ==> -129.57405366310041\n",
      "Episode * 473 * Avg Reward is ==> -128.94043704242208\n",
      "Episode * 474 * Avg Reward is ==> -128.90755501978123\n",
      "Episode * 475 * Avg Reward is ==> -134.2756520691105\n",
      "Episode * 476 * Avg Reward is ==> -136.68273890919394\n",
      "Episode * 477 * Avg Reward is ==> -136.72953856968041\n",
      "Episode * 478 * Avg Reward is ==> -137.6591402843854\n",
      "Episode * 479 * Avg Reward is ==> -133.1082213518343\n",
      "Episode * 480 * Avg Reward is ==> -132.3881483877047\n",
      "Episode * 481 * Avg Reward is ==> -133.04807112119084\n",
      "Episode * 482 * Avg Reward is ==> -132.6469692288354\n",
      "Episode * 483 * Avg Reward is ==> -132.2895060673083\n",
      "Episode * 484 * Avg Reward is ==> -132.0914875235298\n",
      "Episode * 485 * Avg Reward is ==> -131.9568426135625\n",
      "Episode * 486 * Avg Reward is ==> -131.79931848891812\n",
      "Episode * 487 * Avg Reward is ==> -131.99814310174617\n",
      "Episode * 488 * Avg Reward is ==> -132.59725048006618\n",
      "Episode * 489 * Avg Reward is ==> -131.48430490150244\n",
      "Episode * 490 * Avg Reward is ==> -133.9468265328196\n",
      "Episode * 491 * Avg Reward is ==> -134.0186985271443\n",
      "Episode * 492 * Avg Reward is ==> -133.53715584683064\n",
      "Episode * 493 * Avg Reward is ==> -134.46030895483992\n",
      "Episode * 494 * Avg Reward is ==> -134.7124075971445\n",
      "Episode * 495 * Avg Reward is ==> -134.97805792423347\n",
      "Episode * 496 * Avg Reward is ==> -137.83565911327355\n",
      "Episode * 497 * Avg Reward is ==> -138.48507030628338\n",
      "Episode * 498 * Avg Reward is ==> -138.71144230878764\n",
      "Episode * 499 * Avg Reward is ==> -137.28149988892423\n",
      "Episode * 500 * Avg Reward is ==> -137.801403936897\n",
      "Episode * 501 * Avg Reward is ==> -139.78156003613512\n",
      "Episode * 502 * Avg Reward is ==> -139.6329104762188\n",
      "Episode * 503 * Avg Reward is ==> -140.14184710237504\n",
      "Episode * 504 * Avg Reward is ==> -139.58417871514806\n",
      "Episode * 505 * Avg Reward is ==> -139.17319476949768\n",
      "Episode * 506 * Avg Reward is ==> -139.66122911940084\n",
      "Episode * 507 * Avg Reward is ==> -142.18562303361358\n",
      "Episode * 508 * Avg Reward is ==> -141.47755244770593\n",
      "Episode * 509 * Avg Reward is ==> -142.88502790919452\n",
      "Episode * 510 * Avg Reward is ==> -143.52678989449709\n",
      "Episode * 511 * Avg Reward is ==> -142.92117143921485\n"
     ]
    }
   ],
   "source": [
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "\n",
    "# Takes about 4 min to train\n",
    "for ep in range(total_episodes):\n",
    "\n",
    "    prev_state = env.reset()\n",
    "    episodic_reward = 0\n",
    "\n",
    "    while True:\n",
    "        # Uncomment this to see the Actor in action\n",
    "        # But not in a python notebook.\n",
    "        env.render()\n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "\n",
    "        # Recieve state and reward from environment.\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "        update_target(target_actor.variables, actor_model.variables, tau)\n",
    "        update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97d8e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c53733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
