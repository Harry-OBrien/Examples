{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d241a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e1173d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# Own Tensorboard class\n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        self._log_write_dir = self.log_dir\n",
    "\n",
    "    # Overriding this method to stop creating default log writer\n",
    "    def set_model(self, model):\n",
    "        pass\n",
    "\n",
    "    # Overrided, saves logs with our step number\n",
    "    # (otherwise every .fit() will start writing from 0th step)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    # Overrided\n",
    "    # We train for one batch only, no need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Overrided, so won't close writer\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    # Custom method for saving own metrics\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    def update_stats(self, **stats):\n",
    "        with self.writer.as_default():\n",
    "            for key, value in stats.items():\n",
    "                tf.summary.scalar(key, value, step=self.step)\n",
    "                self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b87d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper params\n",
    "# replay_memory_size = 50_000\n",
    "# min_replay_memory_size = 1_000\n",
    "# model_name=\"cart_ddqn\"\n",
    "# seed = 0\n",
    "# learning_rate = 0.001\n",
    "# gamma = 0.99 # Discount factor for past rewards\n",
    "# epsilon = 1.0  # Epsilon greedy parameter\n",
    "# epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "# epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "# epsilon_interval = (epsilon_max - epsilon_min)  # Rate at which to reduce chance of random action being taken\n",
    "# mini_batch_size = 32  # Size of batch taken from replay buffer\n",
    "# max_steps_per_episode = 10000\n",
    "\n",
    "DISCOUNT = 0.99\n",
    "REPLAY_MEMORY_SIZE = 50_000  # How many last steps to keep for model training\n",
    "MIN_REPLAY_MEMORY_SIZE = 100  # Minimum number of steps in a memory to start training\n",
    "MINIBATCH_SIZE = 64  # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
    "MODEL_NAME = 'cart_ddqn'\n",
    "MIN_REWARD = -200  # For model save\n",
    "MEMORY_FRACTION = 0.20\n",
    "\n",
    "# Env settings\n",
    "EPISODES = 100\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 1\n",
    "EPSILON_DECAY = 0.99975\n",
    "EPSILON_MIN = 0.001\n",
    "\n",
    "# Stats settings\n",
    "AGGREGATE_STATS_EVERY = 5 #episodes\n",
    "SHOW_PREVIEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73f2c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN_Agent:\n",
    "    def __init__(self, states, actions, learning_rate):\n",
    "        self.model = self.create_model(states, actions, learning_rate)\n",
    "        self.target_model = self.create_model(states, actions, learning_rate)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.memory = SequentialMemory(limit=REPLAY_MEMORY_SIZE, window_length=1)\n",
    "\n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir=\"logs/{}-{}\".format(MODEL_NAME, int(time.time())))\n",
    "        \n",
    "        self.target_update_counter = 0\n",
    "\n",
    "    def create_model(self, n_states, n_actions, lr):\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(1, n_states)),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(n_actions, activation='linear'),\n",
    "        ])\n",
    "        \n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=lr), metrics=\"accuracy\")        \n",
    "        return model\n",
    "    \n",
    "    # Adds step's data to a memory replay array\n",
    "    # (observation space, action, reward, new observation space, done)\n",
    "    # ARGUMENT:\n",
    "        # Experience: (observation, action, reward, terminal)\n",
    "    def update_replay_memory(self, experience):\n",
    "        self.memory.append(*experience)\n",
    "        \n",
    "    # Queries main network for Q values given current observation space (environment state)\n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]\n",
    "    \n",
    "    def train(self, terminal_state, step):\n",
    "\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if self.memory.nb_entries < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "\n",
    "        # Get a minibatch of random samples from memory replay table\n",
    "        # \n",
    "        minibatch = self.memory.sample(MINIBATCH_SIZE)\n",
    "\n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch])/255\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        # When using target network, query it, otherwise main network should be queried\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])/255\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Now we need to enumerate our batches\n",
    "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            # Update Q value for given state\n",
    "            current_qs = current_qs_list[index][0]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            # And append to our training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "        # Fit on all samples as one batch, log only on terminal state\n",
    "        self.model.fit(np.array(X)/255, np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False, callbacks=[self.tensorboard] if terminal_state else None)\n",
    "\n",
    "        # Update target network counter every episode\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "574dfa39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "agent = DDQN_Agent(\n",
    "    n_states,\n",
    "    n_actions,\n",
    "    learning_rate=0.0003\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50397e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For repeatable results\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# For stats\n",
    "ep_rewards = [-200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "561f8117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-13 15:42:39.806 python[78987:4823455] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fc84fe5ff60>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-01-13 15:42:39.808 python[78987:4823455] Warning: Expected min height of view: (<NSButton: 0x7fc842a6b4e0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-01-13 15:42:39.812 python[78987:4823455] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fc832d1baf0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-01-13 15:42:39.817 python[78987:4823455] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fc832d1c870>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ModifiedTensorBoard' object has no attribute '_train_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kf/01s0w9cs71s3cgq3y74xxtvh0000gn/T/ipykernel_78987/1486556925.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Every step we update replay memory and train the main network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_replay_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kf/01s0w9cs71s3cgq3y74xxtvh0000gn/T/ipykernel_78987/185113695.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, terminal_state, step)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# Fit on all samples as one batch, log only on terminal state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMINIBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mterminal_state\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Update target network counter every episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_train_writer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2250\u001b[0m       self._writers['train'] = tf.summary.create_file_writer(\n\u001b[0;32m-> 2251\u001b[0;31m           self._train_dir)\n\u001b[0m\u001b[1;32m   2252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ModifiedTensorBoard' object has no attribute '_train_dir'"
     ]
    }
   ],
   "source": [
    "for episode in range(1, EPISODES + 1):\n",
    "    # Update tensorboard step every episode\n",
    "    agent.tensorboard.step = episode\n",
    "    \n",
    "    # Restarting episode -> reset ep reward and step\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "    \n",
    "    current_state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "    if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(agent.get_qs(current_state))\n",
    "        else:\n",
    "            # Get random action\n",
    "            action = np.random.randint(0, n_actions)\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Update episodic reward\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if SHOW_PREVIEW:\n",
    "            env.render()\n",
    "            \n",
    "        # Every step we update replay memory and train the main network\n",
    "        agent.update_replay_memory((current_state, action, reward, done))\n",
    "        agent.train(done, step)\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "        \n",
    "    # Append episode reward to a list and log state (every given number of episodes)\n",
    "    ep_rewards.append(episode_reward)\n",
    "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)\n",
    "\n",
    "        # we would save checkpoints here\n",
    "        # ...\n",
    "        \n",
    "    # Finally, handle the epsilon decay\n",
    "    if epsilon > EPSILON_MIN:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(EPSILON_MIN, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b3e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6139764f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c2f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0647e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
