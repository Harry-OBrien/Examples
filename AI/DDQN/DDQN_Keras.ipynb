{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f57072d0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b6050c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import MaxBoltzmannQPolicy\n",
    "from rl.agents.dqn import DQNAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaeba31",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "320fd612",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = 0.99\n",
    "REPLAY_MEM_SIZE = 10_000\n",
    "MIN_MEM_SIZE = 1_000\n",
    "EPISODES = 100_000\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "WINDOW_LENGTH=1\n",
    "SEED = 0\n",
    "MODEL_NAME=\"DDQN_Keras\"\n",
    "UPDATE_TARGET_EVERY=10_000\n",
    "\n",
    "SHOW_PREVIEW = False\n",
    "\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eff49a7",
   "metadata": {},
   "source": [
    "# Modified tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c645b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# Own Tensorboard class\n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        self._log_write_dir = self.log_dir\n",
    "        \n",
    "\n",
    "    # Overriding this method to stop creating default log writer\n",
    "    def set_model(self, model):\n",
    "        pass\n",
    "\n",
    "    # Overrided, saves logs with our step number\n",
    "    # (otherwise every .fit() will start writing from 0th step)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    # Overrided\n",
    "    # We train for one batch only, no need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Overrided, so won't close writer\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    # Custom method for saving own metrics\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    def update_stats(self, **stats):\n",
    "        with self.writer.as_default():\n",
    "            for key, value in stats.items():\n",
    "                tf.summary.scalar(key, value, step=self.step)\n",
    "                self.writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77faee9e",
   "metadata": {},
   "source": [
    "# Our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fbf4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN_Agent:\n",
    "    def __init__(self,\n",
    "                 n_states,\n",
    "                 n_actions,\n",
    "                 lr,\n",
    "                 gamma,\n",
    "                 mem_size,\n",
    "                 min_mem_size,\n",
    "                 model_name,\n",
    "                 update_target_every=2000,\n",
    "                 batch_size=64,\n",
    "                 window_length=1):\n",
    "        \n",
    "#         self.tensorboard = ModifiedTensorBoard(log_dir=\"logs/{}-{}\".format(model_name, int(time.time())))\n",
    "        \n",
    "        model = self.create_model(n_states, n_actions, lr)\n",
    "        policy = MaxBoltzmannQPolicy()\n",
    "        replay_memory = SequentialMemory(limit=mem_size, window_length=window_length)\n",
    "        \n",
    "        self.ddqn = DQNAgent(model=model,\n",
    "                             gamma=gamma,\n",
    "                             policy=policy,\n",
    "                             enable_double_dqn=True,\n",
    "                             memory = replay_memory,\n",
    "                             nb_steps_warmup=min_mem_size,\n",
    "                             batch_size=batch_size,\n",
    "                             target_model_update=update_target_every,\n",
    "                             nb_actions=n_actions)\n",
    "        \n",
    "        self.ddqn.compile(optimizer=Adam(learning_rate=lr), metrics=[\"accuracy\"])\n",
    "        \n",
    "    def fit(self, **kwargs):\n",
    "        self.ddqn.fit(**kwargs)#, callbacks=[self.tensorboard])\n",
    "        \n",
    "    def create_model(self, n_states, n_actions, lr):\n",
    "        model = Sequential([\n",
    "            Flatten(input_shape=(1, n_states)),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(n_actions, activation='linear')\n",
    "        ])\n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71604837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 2)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               384       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,283\n",
      "Trainable params: 17,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-16 17:57:29.537433: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "agent = DDQN_Agent(n_states=n_states,\n",
    "                   n_actions=n_actions,\n",
    "                   lr=LEARNING_RATE,\n",
    "                   gamma=DISCOUNT,\n",
    "                   mem_size=REPLAY_MEM_SIZE,\n",
    "                   min_mem_size=MIN_MEM_SIZE,\n",
    "                   model_name=MODEL_NAME,\n",
    "                   update_target_every=UPDATE_TARGET_EVERY,\n",
    "                   window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cef4b55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "  181/10000 [..............................] - ETA: 5s - reward: -1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 57s 6ms/step - reward: -1.0000\n",
      "50 episodes - episode_reward: -198.780 [-200.000, -139.000] - loss: 0.003 - accuracy: 0.315 - mean_q: -0.968\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: -1.0000\n",
      "50 episodes - episode_reward: -199.000 [-200.000, -150.000] - loss: 0.004 - accuracy: 0.273 - mean_q: -1.927\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -1.0000\n",
      "50 episodes - episode_reward: -199.140 [-200.000, -157.000] - loss: 0.011 - accuracy: 0.332 - mean_q: -2.902\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: -1.0000\n",
      "52 episodes - episode_reward: -194.000 [-200.000, -110.000] - loss: 0.022 - accuracy: 0.278 - mean_q: -3.857\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -1.0000\n",
      "50 episodes - episode_reward: -197.640 [-200.000, -146.000] - loss: 0.037 - accuracy: 0.306 - mean_q: -4.792\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -1.0000\n",
      "50 episodes - episode_reward: -200.000 [-200.000, -200.000] - loss: 0.060 - accuracy: 0.353 - mean_q: -5.729\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -1.0000\n",
      "51 episodes - episode_reward: -199.667 [-200.000, -183.000] - loss: 0.085 - accuracy: 0.420 - mean_q: -6.631\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 60s 6ms/step - reward: -1.0000\n",
      "51 episodes - episode_reward: -194.980 [-200.000, -111.000] - loss: 0.110 - accuracy: 0.368 - mean_q: -7.526\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -1.0000\n",
      "51 episodes - episode_reward: -193.510 [-200.000, -115.000] - loss: 0.124 - accuracy: 0.276 - mean_q: -8.341\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -1.0000\n",
      "done, took 574.144 seconds\n"
     ]
    }
   ],
   "source": [
    "agent.fit(env=env, \n",
    "          nb_steps=EPISODES, \n",
    "          visualize=SHOW_PREVIEW, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7248408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
